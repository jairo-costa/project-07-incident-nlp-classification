{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd094c89",
   "metadata": {},
   "source": [
    "# üß† Project 07 ‚Äî Incident Classification & SLA Risk Detection (NLP)\n",
    "### **Study Notebook ‚Äî Guided Exploration & Mentorship Version**\n",
    "\n",
    "---\n",
    "\n",
    "## üìå What this notebook is\n",
    "\n",
    "This is the **study-version notebook**, where the full reasoning process is documented:\n",
    "\n",
    "- exploratory steps  \n",
    "- trial and error  \n",
    "- debugging  \n",
    "- alternative modeling paths  \n",
    "- detailed explanations  \n",
    "- mentor questions & reflections  \n",
    "- incremental improvements  \n",
    "\n",
    "It captures the *real learning journey* behind the project ‚Äî not only the final result.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Summary\n",
    "\n",
    "The goal of this project is to build and compare NLP pipelines to automatically classify **logistics incident reports** that impact SLA performance.  \n",
    "We will evaluate classical ML models (TF-IDF + classifiers) alongside modern embedding-based approaches.\n",
    "\n",
    "Key outcomes:\n",
    "\n",
    "- Structured NLP preprocessing workflow  \n",
    "- Incident classification models  \n",
    "- Operational insights relevant for SLA management  \n",
    "- Comparison between modeling strategies  \n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Notebook Structure (study version)\n",
    "\n",
    "1. **PPI ‚Äî Preliminary Problem Identification**  \n",
    "2. **Data Loading & Initial Inspection**  \n",
    "3. **Text Exploration & Cleaning Strategy**  \n",
    "4. **NLP Preprocessing Pipeline**  \n",
    "5. **TF-IDF Vectorization + Classical Models**  \n",
    "6. **Embedding-Based Classification**  \n",
    "7. **Model Comparison (Metrics + Interpretability)**  \n",
    "8. **Operational Insights**  \n",
    "9. **Next Steps toward final-version notebook**\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Notes\n",
    "\n",
    "This notebook is intentionally verbose.  \n",
    "Every decision is explained.  \n",
    "Every mistake stays visible until corrected.  \n",
    "Every question raised here will shape the final executive version.\n",
    "\n",
    "If you're reading this as part of the portfolio:  \n",
    "üëâ *The final clean deliverable is located in* `notebooks/final-version.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs start the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063d0e0",
   "metadata": {},
   "source": [
    "# üß† Mentorship Block ‚Äî Guided Reasoning Before Analysis\n",
    "\n",
    "This section contains conceptual and strategic questions designed to:\n",
    "- strengthen my reasoning before writing code,  \n",
    "- clarify expectations about the data,  \n",
    "- refine my understanding of NLP in operational environments,  \n",
    "- anticipate challenges,  \n",
    "- and prepare me for the analytical phase.\n",
    "\n",
    "These answers will influence:\n",
    "- my PPI refinement,  \n",
    "- the modeling strategy,  \n",
    "- the experiment plan,  \n",
    "- and the final-version notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## **1) What operational scenarios do I imagine when reading an incident report?**\n",
    "\n",
    "*(Describe a few hypothetical incident messages and what they imply operationally.)*\n",
    "\n",
    "During the review of the incident report, the following operational scenarios came to mind:\n",
    "\n",
    "Route Obstruction:\n",
    "Ongoing road construction led to two streets along the planned delivery route being blocked for an indefinite period, requiring immediate rerouting.\n",
    "\n",
    "System / Connectivity Failure:\n",
    "An internet connectivity issue prevented the GPS application from updating in real time, causing inaccurate tracking information and operational delays.\n",
    "\n",
    "Logistics Capacity Constraint:\n",
    "A shipment remained at the distribution center for three consecutive days due to the lack of an available vehicle assigned to that route.\n",
    "\n",
    "Vehicle Breakdown:\n",
    "A delivery vehicle experienced a mechanical failure due to insufficient preventive maintenance, rendering it non-operational and interrupting the delivery sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **2) Which incident types do I believe are the most critical for SLA? Why?**\n",
    "\n",
    "**The most critical incident types for SLA performance, in order of severity, are:**\n",
    "\n",
    "1. **Vehicle breakdowns (especially due to lack of maintenance)**  \n",
    "   These incidents immediately interrupt the delivery chain and can halt an entire route.  \n",
    "   A breakdown typically creates cascading delays, affects multiple orders, and has no quick workaround unless contingency vehicles or repair teams are readily available.  \n",
    "   This makes it one of the highest-impact events for SLA compliance.\n",
    "\n",
    "2. **Shipments stuck at the distribution center for extended periods**  \n",
    "   When a package remains in the hub for several days, the SLA is already at risk before the delivery even begins.  \n",
    "   Regardless of the root cause, prolonged stagnation indicates a structural failure in capacity planning or resource allocation, making it an operational red flag.\n",
    "\n",
    "3. **Internet or system connectivity failures**  \n",
    "   These incidents disrupt real-time tracking, routing updates, and communication with drivers.  \n",
    "   Although typically resolvable, they degrade visibility and decision-making, increasing the likelihood of SLA breaches due to delayed responses.\n",
    "\n",
    "4. **Road construction or route blockages**  \n",
    "   These events require rerouting and may add significant travel time, but they are generally predictable and can often be mitigated with advance planning or updated routing strategies.  \n",
    "   As a result, their SLA impact is lower compared to system failures or vehicle breakdowns.\n",
    "\n",
    "---\n",
    "\n",
    "## **3) If I had to manually classify 300 incident messages per day, what patterns would I look for?**\n",
    "\n",
    "If I were manually classifying a large volume of incident messages, I would look for specific keywords and linguistic patterns that indicate the nature and urgency of the issue. These terms act as strong semantic signals for operational categorization.\n",
    "\n",
    "Examples include:\n",
    "\n",
    "- **‚Äúvehicle breakdown‚Äù, ‚Äúmechanical failure‚Äù, ‚Äúengine issue‚Äù**  \n",
    "  ‚Üí Indicates a vehicle-related incident with high SLA risk.\n",
    "\n",
    "- **‚Äúdelivery delay‚Äù, ‚Äúrunning late‚Äù, ‚Äúbehind schedule‚Äù**  \n",
    "  ‚Üí Suggests time-related issues affecting route performance.\n",
    "\n",
    "- **‚Äúunable to deliver‚Äù, ‚Äúcustomer not found‚Äù, ‚Äúaddress issue‚Äù**  \n",
    "  ‚Üí Points to delivery exceptions that may require reattempt or rerouting.\n",
    "\n",
    "- **‚Äúroad blocked‚Äù, ‚Äúconstruction‚Äù, ‚Äúroute closed‚Äù, ‚Äúobstruction‚Äù**  \n",
    "  ‚Üí Signals external route disruptions that impact travel time.\n",
    "\n",
    "- **‚Äúvehicle maintenance‚Äù, ‚Äúneeds repair‚Äù, ‚Äúinspection required‚Äù**  \n",
    "  ‚Üí Suggests structural issues that may affect fleet availability.\n",
    "\n",
    "- **‚Äúshipment stuck‚Äù, ‚Äúorder held‚Äù, ‚Äúitem not dispatched‚Äù**  \n",
    "  ‚Üí Indicates stagnation at the distribution center, a strong SLA warning.\n",
    "\n",
    "- **‚Äúno tracking‚Äù, ‚Äúsystem not updating‚Äù, ‚ÄúGPS error‚Äù**  \n",
    "  ‚Üí Implies system or connectivity issues that reduce visibility and response speed.\n",
    "\n",
    "These keyword patterns help identify the probable incident type quickly, even before examining the full context.\n",
    "\n",
    "---\n",
    "\n",
    "## **4) What kinds of text noise do I expect?**\n",
    "\n",
    "By understanding the context of the possible message sender ‚Äî including the operational situation, sense of urgency, and commitment to keeping the operation running ‚Äî I expect to encounter several types of text noise in the incident reports.\n",
    "\n",
    "These include:\n",
    "\n",
    "### **Typos and misspellings**\n",
    "Often caused by haste, mobile typing, or voice-to-text errors:\n",
    "- ‚Äúvihacles nreakdaw‚Äù\n",
    "- ‚Äúfail car‚Äù\n",
    "- ‚Äúingene poblem‚Äù\n",
    "\n",
    "### **Incomplete or truncated messages**\n",
    "Typically written under time pressure or connectivity issues:\n",
    "- ‚Äúcannot deliv...‚Äù\n",
    "- ‚Äúintercep...‚Äù\n",
    "- ‚Äúdelay...‚Äù\n",
    "\n",
    "### **Role-specific operational vocabulary**\n",
    "Messages may contain terms, abbreviations, or jargon commonly used by drivers, dispatchers, or hub operators, which may not follow standard language conventions.\n",
    "\n",
    "### **Slang and functional expressions**\n",
    "Informal or shorthand expressions used to communicate quickly within the operation, often relying on shared contextual knowledge rather than grammatical accuracy.\n",
    "\n",
    "These characteristics reflect real-world operational communication and reinforce the need for robust NLP preprocessing rather than rigid keyword rules.\n",
    "\n",
    "---\n",
    "\n",
    "## **5) What makes NLP necessary here instead of simple keyword rules?**\n",
    "\n",
    "NLP is essential in this scenario because it can account for multiple factors that influence how operational messages are written and interpreted.\n",
    "\n",
    "By processing natural language, NLP models are able to:\n",
    "- capture linguistic patterns and behavioral signals present in human communication,\n",
    "- distinguish contextual meaning, reducing ambiguity and avoiding double or multiple interpretations,\n",
    "- identify the underlying intent behind incident messages rather than relying solely on isolated keywords.\n",
    "\n",
    "This contextual understanding allows the model to reduce category overlap and misclassification.\n",
    "By evaluating language patterns together with operational behavior, NLP significantly increases the accuracy and reliability of incident interpretation compared to rigid, rule-based approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## **6) How do I think class imbalance will appear in this dataset?**\n",
    "\n",
    "Proper incident classification is critical to enable action plans that are aligned with both the urgency and the frequency of each event.\n",
    "\n",
    "By classifying incidents accurately, the operation can:\n",
    "- define response priorities based on risk to SLA,\n",
    "- allocate resources more efficiently,\n",
    "- distinguish between recurring operational issues and rare but critical events,\n",
    "- and ensure that high-impact incidents receive immediate attention.\n",
    "\n",
    "Without structured classification, all incidents tend to be treated similarly, which leads to delayed responses, inefficient prioritization, and increased operational risk.\n",
    "Therefore, classification is not only a technical task but a key decision-support mechanism for operational management.\n",
    "\n",
    "---\n",
    "\n",
    "## **7) Do I expect short or long descriptions? Why?**\n",
    "\n",
    "Both short and long incident descriptions are expected, depending on the severity and complexity of the problem.\n",
    "\n",
    "For example, mechanical issues often require longer descriptions so that the resolution team has full context regarding the failure, symptoms, and constraints. This additional detail helps accelerate diagnosis and reduce downtime.\n",
    "\n",
    "In contrast, route obstructions or external events such as road closures can usually be described briefly, as a short message is often sufficient to communicate the issue and trigger a rerouting decision.\n",
    "\n",
    "---\n",
    "\n",
    "## **8) Which classification errors are the most dangerous for SLA?**\n",
    "\n",
    "One of the most harmful classification errors is treating incidents of different complexity as if they were equivalent.\n",
    "\n",
    "For instance:\n",
    "- grouping all mechanical failures under the same category ignores differences in severity and recovery time;\n",
    "- failing to distinguish between a complete road closure and a temporary traffic congestion can lead to incorrect prioritization.\n",
    "\n",
    "Such misclassifications may delay critical responses, allocate resources inefficiently, and ultimately increase the risk of SLA breaches.\n",
    "\n",
    "---\n",
    "\n",
    "## **9) When comparing TF-IDF vs Embeddings, what differences do I expect?**\n",
    "\n",
    "I expect the models to provide a deeper level of insight beyond simple categorization.\n",
    "\n",
    "This includes:\n",
    "- a more accurate evaluation of context,\n",
    "- better interpretation of operational scenarios,\n",
    "- and clearer differentiation between incident severity levels.\n",
    "\n",
    "With richer contextual understanding, the classification results become more reliable and actionable, directly supporting better decision-making and faster operational responses.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Notes  \n",
    "These questions shape the analytical mindset required for NLP-based operational modeling.  \n",
    "They help establish intuition before we work with real data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fcb5a",
   "metadata": {},
   "source": [
    "# üß© PPI ‚Äî Preliminary Problem Identification  \n",
    "### *Refined understanding after guided mentorship*\n",
    "\n",
    "This section consolidates my refined understanding of the problem after the mentorship phase.  \n",
    "The goal is to clearly define the nature of the business challenge, the data, and the analytical approach before any modeling decisions are made.\n",
    "\n",
    "---\n",
    "\n",
    "## **1) What is the problem asking?**\n",
    "\n",
    "The problem asks for the development of an NLP-based system capable of automatically classifying operational incident reports in logistics operations.\n",
    "\n",
    "The objective is not only to categorize incidents, but to support faster and more accurate operational decision-making, especially in scenarios that pose a risk to Service Level Agreements (SLA).\n",
    "\n",
    "By structuring and interpreting textual incident descriptions, the model should help prioritize responses, allocate resources efficiently, and reduce operational delays.\n",
    "\n",
    "---\n",
    "\n",
    "## **2) What type of data do I expect to receive?**\n",
    "\n",
    "I expect primarily unstructured textual data containing incident descriptions written by different operational actors such as drivers, dispatchers, and hub operators.\n",
    "\n",
    "The data is likely to include:\n",
    "- short and long descriptions,\n",
    "- informal language,\n",
    "- typos and abbreviations,\n",
    "- role-specific operational vocabulary,\n",
    "- incomplete or truncated messages caused by urgency or connectivity issues.\n",
    "\n",
    "Additional metadata (such as timestamps or categories) may exist, but the core challenge lies in understanding the text itself.\n",
    "\n",
    "---\n",
    "\n",
    "## **3) Is this a supervised or unsupervised problem? Why?**\n",
    "\n",
    "This is a **supervised learning problem**.\n",
    "\n",
    "The goal is to map incident descriptions to predefined categories or severity levels based on historical examples.  \n",
    "Each message is expected to have an associated label that represents the incident type, urgency, or operational impact, which enables model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **4) Why is NLP appropriate for this scenario?**\n",
    "\n",
    "NLP is appropriate because operational incident reports contain contextual meaning that cannot be reliably captured using simple rules or keyword matching.\n",
    "\n",
    "Through NLP techniques, it is possible to:\n",
    "- interpret context rather than isolated words,\n",
    "- reduce ambiguity and overlapping meanings,\n",
    "- identify the intent behind messages,\n",
    "- and handle linguistic variability caused by human communication under pressure.\n",
    "\n",
    "This makes NLP far more effective than rigid, rule-based approaches for real operational environments.\n",
    "\n",
    "---\n",
    "\n",
    "## **5) What are the main challenges I anticipate?**\n",
    "\n",
    "Key challenges include:\n",
    "- ambiguous language and overlapping incident categories,\n",
    "- noisy text with typos and informal expressions,\n",
    "- class imbalance between frequent and rare but critical incidents,\n",
    "- short messages that lack explicit context,\n",
    "- and the need to balance interpretability with model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **6) What approaches do I believe are worth testing?**\n",
    "\n",
    "Before exploring the dataset, I expect to evaluate and compare:\n",
    "\n",
    "**Classical NLP approaches:**\n",
    "- TF-IDF vectorization combined with Logistic Regression\n",
    "- TF-IDF combined with tree-based models\n",
    "\n",
    "**Modern approaches:**\n",
    "- Embedding-based representations with similarity or classification layers\n",
    "- Comparative analysis between classical and embedding-based methods\n",
    "\n",
    "The comparison will focus not only on performance metrics, but also on interpretability and operational applicability.\n",
    "\n",
    "---\n",
    "\n",
    "## **7) Which evaluation metrics matter most for the business?**\n",
    "\n",
    "Beyond overall accuracy, metrics such as:\n",
    "- Recall per class,\n",
    "- F1-score,\n",
    "- and confusion matrices\n",
    "\n",
    "are critical, especially for high-impact incident categories.  \n",
    "Misclassifying severe incidents as low priority poses a much greater SLA risk than the opposite error.\n",
    "\n",
    "---\n",
    "\n",
    "## **8) What insights do I expect to extract from this project?**\n",
    "\n",
    "I expect to gain insights into:\n",
    "- recurring operational failure patterns,\n",
    "- incident types with the highest SLA risk,\n",
    "- how language structure correlates with urgency,\n",
    "- and which modeling approach provides the best balance between accuracy, robustness, and explainability.\n",
    "\n",
    "These insights should directly support operational monitoring and faster decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Notes  \n",
    "This PPI represents my consolidated understanding after mentorship.  \n",
    "All assumptions will be validated or challenged during exploratory analysis and modeling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12146be",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Problem Framing & Label Definition  \n",
    "### *Defining what we are predicting before touching the data*\n",
    "\n",
    "Before loading the dataset, it is critical to clearly define the classification problem and the target labels.  \n",
    "Well-defined labels reduce ambiguity, improve model performance, and ensure that the results are meaningful for operational decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## **1) What exactly is the prediction task?**\n",
    "\n",
    "The task is a **multiclass text classification problem**.\n",
    "\n",
    "Given a textual incident description, the model must predict:\n",
    "- the **type of operational incident**,  \n",
    "- and implicitly support **SLA risk prioritization**.\n",
    "\n",
    "Each input consists of a free-text message describing an incident.  \n",
    "Each output corresponds to a predefined incident category.\n",
    "\n",
    "---\n",
    "\n",
    "## **2) Why multiclass classification (and not binary)?**\n",
    "\n",
    "A binary setup (e.g., *critical vs. non-critical*) would oversimplify the operational reality.\n",
    "\n",
    "Different incident types:\n",
    "- require different response teams,\n",
    "- have different resolution times,\n",
    "- and pose different levels of SLA risk.\n",
    "\n",
    "Multiclass classification allows the model to preserve this operational nuance and support more precise action plans.\n",
    "\n",
    "---\n",
    "\n",
    "## **3) Proposed incident categories (labels)**\n",
    "\n",
    "Based on operational reasoning and mentorship insights, the initial label set may include:\n",
    "\n",
    "- **Vehicle Breakdown / Mechanical Failure**  \n",
    "  Incidents related to vehicle malfunction, maintenance issues, or breakdowns.\n",
    "\n",
    "- **Route Obstruction / External Disruption**  \n",
    "  Road closures, construction, accidents, or environmental factors affecting routes.\n",
    "\n",
    "- **System / Connectivity Failure**  \n",
    "  GPS issues, tracking failures, system downtime, or internet connectivity problems.\n",
    "\n",
    "- **Shipment Delay / Stagnation at Hub**  \n",
    "  Orders stuck at distribution centers due to capacity or planning constraints.\n",
    "\n",
    "- **Other / Miscellaneous**  \n",
    "  Incidents that do not clearly fit into the main categories.\n",
    "\n",
    "‚ö†Ô∏è These labels are **initial hypotheses** and may be refined after data exploration.\n",
    "\n",
    "---\n",
    "\n",
    "## **4) What does each label represent operationally?**\n",
    "\n",
    "Each label corresponds to a distinct operational response:\n",
    "\n",
    "- **Mechanical failures** ‚Üí Fleet maintenance or contingency vehicles  \n",
    "- **Route obstructions** ‚Üí Rerouting and dispatch coordination  \n",
    "- **System failures** ‚Üí IT or monitoring intervention  \n",
    "- **Shipment stagnation** ‚Üí Capacity planning and resource reallocation  \n",
    "\n",
    "This mapping reinforces that classification supports *action*, not just categorization.\n",
    "\n",
    "---\n",
    "\n",
    "## **5) Expected label challenges**\n",
    "\n",
    "Potential challenges include:\n",
    "- overlapping language between categories,\n",
    "- incidents that combine multiple issues,\n",
    "- imbalance between frequent and rare but critical events,\n",
    "- ambiguous or incomplete descriptions.\n",
    "\n",
    "These challenges will directly influence preprocessing choices and model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **6) Success criteria for labeling**\n",
    "\n",
    "Labeling will be considered successful if:\n",
    "- categories are operationally meaningful,\n",
    "- misclassifications do not hide critical incidents,\n",
    "- results are interpretable by non-technical stakeholders,\n",
    "- and the model supports faster SLA-aware decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Notes  \n",
    "This framing defines the backbone of the project.  \n",
    "All modeling, metrics, and comparisons will be aligned with these labels.\n",
    "\n",
    "The label structure may be revisited after exploratory analysis if the data reveals new patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a1cd89",
   "metadata": {},
   "source": [
    "# üì¶ Dataset Selection & Assumptions  \n",
    "### *Understanding the data source before loading it*\n",
    "\n",
    "Before loading the dataset, it is important to document the rationale behind its selection and the assumptions made about its structure and quality.  \n",
    "This step helps contextualize the analysis and sets realistic expectations for the modeling phase.\n",
    "\n",
    "---\n",
    "\n",
    "## **1) Why this dataset?**\n",
    "\n",
    "The selected dataset represents incident reports written in natural language within a logistics or operational context.\n",
    "\n",
    "It was chosen because it:\n",
    "- contains free-text descriptions similar to real operational messages,\n",
    "- reflects the type of language used by drivers, hubs, and monitoring teams,\n",
    "- aligns with the business objective of classifying incidents that impact SLA,\n",
    "- allows experimentation with both classical NLP and embedding-based approaches.\n",
    "\n",
    "Even as a sample or synthetic dataset, it simulates real-world challenges commonly found in operational text data.\n",
    "\n",
    "---\n",
    "\n",
    "## **2) What do I assume about the dataset structure?**\n",
    "\n",
    "Before inspection, I assume the dataset includes:\n",
    "- a textual field describing the incident (unstructured text),\n",
    "- a categorical label representing the incident type,\n",
    "- potentially additional metadata such as timestamps or IDs.\n",
    "\n",
    "The text field is expected to be the primary input for the NLP pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## **3) What assumptions am I making about the text data?**\n",
    "\n",
    "Based on operational reasoning, I assume that:\n",
    "- messages vary significantly in length,\n",
    "- descriptions may be informal or incomplete,\n",
    "- spelling errors and abbreviations are common,\n",
    "- similar incidents may be described using different wording,\n",
    "- multiple incident types may share overlapping vocabulary.\n",
    "\n",
    "These assumptions will guide preprocessing decisions such as normalization, tokenization, and vectorization.\n",
    "\n",
    "---\n",
    "\n",
    "## **4) What assumptions am I making about the labels?**\n",
    "\n",
    "At this stage, I assume that:\n",
    "- labels represent meaningful operational categories,\n",
    "- each message is associated with a single primary label,\n",
    "- some classes may be underrepresented,\n",
    "- labeling may contain noise or borderline cases.\n",
    "\n",
    "These assumptions will be validated during exploratory analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **5) Known limitations at this stage**\n",
    "\n",
    "Before inspecting the data, I acknowledge that:\n",
    "- the dataset may not cover all real-world incident variations,\n",
    "- class distribution may be imbalanced,\n",
    "- labels may oversimplify complex operational scenarios.\n",
    "\n",
    "These limitations will be explicitly considered when evaluating model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úîÔ∏è Notes  \n",
    "All assumptions listed here are hypotheses.  \n",
    "They exist to guide the analysis, not to constrain it.\n",
    "\n",
    "Once the dataset is loaded and explored, these assumptions will be revisited, validated, or adjusted accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a805628",
   "metadata": {},
   "source": [
    "# üì• Data Loading & Initial Inspection  \n",
    "### *First contact with the dataset*\n",
    "\n",
    "In this step, the goal is to load the dataset and perform a high-level inspection to understand its structure, size, and basic characteristics.\n",
    "\n",
    "At this stage, we intentionally avoid transformations or preprocessing.\n",
    "The focus is on answering fundamental questions such as:\n",
    "- How many records are available?\n",
    "- Which columns exist?\n",
    "- What is the primary text field?\n",
    "- Are there obvious missing values?\n",
    "- Do labels appear consistent with our assumptions?\n",
    "\n",
    "This initial inspection will validate (or challenge) the assumptions defined in the previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbe1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.10.8)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (run once)\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783b178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7314/7314 [00:00<00:00, 55310.30 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'sub_issues_summary', 'active_lock_reason', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "        num_rows: 7314\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"25b3nk/github-issues\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41550bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>body</th>\n",
       "      <th>closed_by</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>is_pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2802957388</td>\n",
       "      <td>I_kwDODunzps6nEbxM</td>\n",
       "      <td>7378</td>\n",
       "      <td>Allow pushing config version to hub</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>### Feature request\\n\\nCurrently, when dataset...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2802723285</td>\n",
       "      <td>I_kwDODunzps6nDinV</td>\n",
       "      <td>7377</td>\n",
       "      <td>Support for sparse arrays with the Arrow Spars...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>### Feature request\\n\\nAI in biology is becomi...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7376</td>\n",
       "      <td>2802621104</td>\n",
       "      <td>PR_kwDODunzps6IiO9j</td>\n",
       "      <td>7376</td>\n",
       "      <td>[docs] uv install</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Proposes adding uv to installation docs (see S...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'diff_url': 'https://github.com/huggingface/d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2800609218</td>\n",
       "      <td>I_kwDODunzps6m7efC</td>\n",
       "      <td>7375</td>\n",
       "      <td>vllmÊâπÈáèÊé®ÁêÜÊä•Èîô</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\n![Image](https://githu...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7374</td>\n",
       "      <td>2793442320</td>\n",
       "      <td>PR_kwDODunzps6IC66n</td>\n",
       "      <td>7374</td>\n",
       "      <td>Remove .h5 from imagefolder extensions</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>the format is not relevant for imagefolder, an...</td>\n",
       "      <td>{'avatar_url': 'https://avatars.githubusercont...</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'diff_url': 'https://github.com/huggingface/d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "4  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/issues...  2802957388   \n",
       "1  https://github.com/huggingface/datasets/issues...  2802723285   \n",
       "2  https://github.com/huggingface/datasets/pull/7376  2802621104   \n",
       "3  https://github.com/huggingface/datasets/issues...  2800609218   \n",
       "4  https://github.com/huggingface/datasets/pull/7374  2793442320   \n",
       "\n",
       "               node_id  number  \\\n",
       "0   I_kwDODunzps6nEbxM    7378   \n",
       "1   I_kwDODunzps6nDinV    7377   \n",
       "2  PR_kwDODunzps6IiO9j    7376   \n",
       "3   I_kwDODunzps6m7efC    7375   \n",
       "4  PR_kwDODunzps6IC66n    7374   \n",
       "\n",
       "                                               title  ... active_lock_reason  \\\n",
       "0                Allow pushing config version to hub  ...               None   \n",
       "1  Support for sparse arrays with the Arrow Spars...  ...               None   \n",
       "2                                  [docs] uv install  ...               None   \n",
       "3                                         vllmÊâπÈáèÊé®ÁêÜÊä•Èîô  ...               None   \n",
       "4             Remove .h5 from imagefolder extensions  ...               None   \n",
       "\n",
       "                                                body  \\\n",
       "0  ### Feature request\\n\\nCurrently, when dataset...   \n",
       "1  ### Feature request\\n\\nAI in biology is becomi...   \n",
       "2  Proposes adding uv to installation docs (see S...   \n",
       "3  ### Describe the bug\\n\\n![Image](https://githu...   \n",
       "4  the format is not relevant for imagefolder, an...   \n",
       "\n",
       "                                           closed_by  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  {'avatar_url': 'https://avatars.githubusercont...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "1  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "2  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "3  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "4  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "1  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "2  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "3  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "4  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "\n",
       "  state_reason draft                                       pull_request  \\\n",
       "0         None   NaN                                               None   \n",
       "1         None   NaN                                               None   \n",
       "2         None   0.0  {'diff_url': 'https://github.com/huggingface/d...   \n",
       "3         None   NaN                                               None   \n",
       "4         None   0.0  {'diff_url': 'https://github.com/huggingface/d...   \n",
       "\n",
       "  is_pull_request  \n",
       "0           False  \n",
       "1           False  \n",
       "2            True  \n",
       "3           False  \n",
       "4            True  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(ds[\"train\"])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ed5faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size (bytes): 0\n",
      "\n",
      "--- head(3) ---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"../data/raw/incidents_sample.csv\")\n",
    "print(\"Exists:\", p.exists())\n",
    "print(\"Size (bytes):\", p.stat().st_size if p.exists() else None)\n",
    "\n",
    "# Optional: preview first 3 lines\n",
    "if p.exists():\n",
    "    print(\"\\n--- head(3) ---\")\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for _ in range(3):\n",
    "            print(f.readline().rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c48021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>body</th>\n",
       "      <th>closed_by</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>is_pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2802957388</td>\n",
       "      <td>I_kwDODunzps6nEbxM</td>\n",
       "      <td>7378</td>\n",
       "      <td>Allow pushing config version to hub</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>### Feature request\\n\\nCurrently, when dataset...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2802723285</td>\n",
       "      <td>I_kwDODunzps6nDinV</td>\n",
       "      <td>7377</td>\n",
       "      <td>Support for sparse arrays with the Arrow Spars...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>### Feature request\\n\\nAI in biology is becomi...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7376</td>\n",
       "      <td>2802621104</td>\n",
       "      <td>PR_kwDODunzps6IiO9j</td>\n",
       "      <td>7376</td>\n",
       "      <td>[docs] uv install</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Proposes adding uv to installation docs (see S...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'diff_url': 'https://github.com/huggingface/d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2800609218</td>\n",
       "      <td>I_kwDODunzps6m7efC</td>\n",
       "      <td>7375</td>\n",
       "      <td>vllmÊâπÈáèÊé®ÁêÜÊä•Èîô</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\n![Image](https://githu...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7374</td>\n",
       "      <td>2793442320</td>\n",
       "      <td>PR_kwDODunzps6IC66n</td>\n",
       "      <td>7374</td>\n",
       "      <td>Remove .h5 from imagefolder extensions</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>the format is not relevant for imagefolder, an...</td>\n",
       "      <td>{'avatar_url': 'https://avatars.githubusercont...</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'diff_url': 'https://github.com/huggingface/d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "4  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/issues...  2802957388   \n",
       "1  https://github.com/huggingface/datasets/issues...  2802723285   \n",
       "2  https://github.com/huggingface/datasets/pull/7376  2802621104   \n",
       "3  https://github.com/huggingface/datasets/issues...  2800609218   \n",
       "4  https://github.com/huggingface/datasets/pull/7374  2793442320   \n",
       "\n",
       "               node_id  number  \\\n",
       "0   I_kwDODunzps6nEbxM    7378   \n",
       "1   I_kwDODunzps6nDinV    7377   \n",
       "2  PR_kwDODunzps6IiO9j    7376   \n",
       "3   I_kwDODunzps6m7efC    7375   \n",
       "4  PR_kwDODunzps6IC66n    7374   \n",
       "\n",
       "                                               title  ... active_lock_reason  \\\n",
       "0                Allow pushing config version to hub  ...               None   \n",
       "1  Support for sparse arrays with the Arrow Spars...  ...               None   \n",
       "2                                  [docs] uv install  ...               None   \n",
       "3                                         vllmÊâπÈáèÊé®ÁêÜÊä•Èîô  ...               None   \n",
       "4             Remove .h5 from imagefolder extensions  ...               None   \n",
       "\n",
       "                                                body  \\\n",
       "0  ### Feature request\\n\\nCurrently, when dataset...   \n",
       "1  ### Feature request\\n\\nAI in biology is becomi...   \n",
       "2  Proposes adding uv to installation docs (see S...   \n",
       "3  ### Describe the bug\\n\\n![Image](https://githu...   \n",
       "4  the format is not relevant for imagefolder, an...   \n",
       "\n",
       "                                           closed_by  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  {'avatar_url': 'https://avatars.githubusercont...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "1  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "2  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "3  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "4  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "1  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "2  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "3  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "4  https://api.github.com/repos/huggingface/datas...                     None   \n",
       "\n",
       "  state_reason draft                                       pull_request  \\\n",
       "0         None   NaN                                               None   \n",
       "1         None   NaN                                               None   \n",
       "2         None   0.0  {'diff_url': 'https://github.com/huggingface/d...   \n",
       "3         None   NaN                                               None   \n",
       "4         None   0.0  {'diff_url': 'https://github.com/huggingface/d...   \n",
       "\n",
       "  is_pull_request  \n",
       "0           False  \n",
       "1           False  \n",
       "2            True  \n",
       "3           False  \n",
       "4            True  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic inspection\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ce60c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'incident_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'incident_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Character and word count\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mchar_length\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mincident_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m).apply(\u001b[38;5;28mlen\u001b[39m)\n\u001b[32m      3\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mword_count\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mincident_text\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x.split()))\n\u001b[32m      5\u001b[39m df[[\u001b[33m\"\u001b[39m\u001b[33mchar_length\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mword_count\u001b[39m\u001b[33m\"\u001b[39m]].describe()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'incident_text'"
     ]
    }
   ],
   "source": [
    "# Character and word count\n",
    "df[\"char_length\"] = df[\"incident_text\"].astype(str).apply(len)\n",
    "df[\"word_count\"] = df[\"incident_text\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "df[[\"char_length\", \"word_count\"]].describe()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
